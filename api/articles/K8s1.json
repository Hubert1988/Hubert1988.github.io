{"title":"K8s","uid":"8b277fff122412f48b0619581df54238","slug":"K8s1","date":"2021-08-26T10:49:36.000Z","updated":"2022-05-05T11:54:05.126Z","comments":true,"path":"api/articles/K8s1.json","keywords":null,"cover":"https://pic4.zhimg.com/v2-562267b2cf39fded4c66640ac37ee818_1440w.jpg?source=172ae18b","content":"<h1 id=\"K8s\"><a href=\"#K8s\" class=\"headerlink\" title=\"K8s\"></a>K8s</h1><h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><h3 id=\"前置知识\"><a href=\"#前置知识\" class=\"headerlink\" title=\"前置知识\"></a>前置知识</h3><ul>\n<li><strong>容器的本质是一种特殊的进程</strong> </li>\n<li>容器的<strong>隔离</strong>和<strong>资源限制</strong>实现<ol>\n<li><strong>namespace</strong><ul>\n<li>相当于一个障眼法，利用linux的namespace机制实现隔离，而一个个docker实际上是一个个特殊的进程</li>\n<li><strong>Namespace 技术实际上修改了应用进程看待整个计算机“视 图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来 说，这些被“隔离”了的进程跟其他进程并没有太大区别。</strong> </li>\n<li>namespace的实现有很多类，比如PID，mount，network等方式（这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的 改变，一定是伴随着挂载操作（mount）才能生效。 ）</li>\n</ul>\n</li>\n<li><strong>cgroups</strong><ul>\n<li>Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够 使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 </li>\n<li><strong>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织 在操作系统的 &#x2F;sys&#x2F;fs&#x2F;cgroup 路径下</strong> </li>\n<li>在 &#x2F;sys&#x2F;fs&#x2F;cgroup目录下又很多资源，比如CPU，内存，如果我们在对应文件里面创建一个进程对应的文件，文件会自动创建一些目录，对响应的目录进行设置即可对资源进行限制，最后在将进程的PID写入task文件即可</li>\n</ul>\n</li>\n<li><strong>容器的文件系统通过chroot进行改变挂载根目录实现</strong>（还不是特别懂）</li>\n</ol>\n</li>\n<li>容器的安全问题<ul>\n<li>在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。因为本质来讲就是一个进程，公用同一个内核，所以安全性不高。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"是什么\"><a href=\"#是什么\" class=\"headerlink\" title=\"是什么\"></a>是什么</h3><ul>\n<li>Kubernetes 是一个容器的编排工具 <strong>（用来管理和编排容器的）</strong>可以理解为云计算的操作系统</li>\n</ul>\n<h3 id=\"浅析K8s\"><a href=\"#浅析K8s\" class=\"headerlink\" title=\"浅析K8s\"></a>浅析K8s</h3><ul>\n<li>技术本质：<ul>\n<li><strong>容器本身没有价值，有价值的是“容器编排</strong> ，容器本身是一个底层技术，解决了应用打包这个问题，但是技术要落地到应用，要落地到Pass平台，所以如何利用容器进行商业化的应用才是真正的价值。而容器编排技术正是Pass落地的关键。</li>\n</ul>\n</li>\n<li>核心特点：<ul>\n<li>容器编排技术有很多，比如Docker Swarm 和 Mesos ，他们都能达到容器编排的目的，但是他们编排技术的出发点不同， Swarm是以docker为核心进行开发，而K8s是借鉴了谷歌的Brog项目，从顶层开始设计，docker仅仅是设计中的一部分，，Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各 种关系，并且为将来支持更多种类的关系留有余地。 而不仅仅是围绕着docker进行发展。</li>\n</ul>\n</li>\n<li>价值：<ul>\n<li>Kubernetes 项目为用户提供的不仅限于一个工具。它真正的价值，乃在于提供 了<strong>一套基于容器构建分布式系统的基础依赖</strong>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"核心组件\"><a href=\"#核心组件\" class=\"headerlink\" title=\"核心组件\"></a>核心组件</h3><ul>\n<li><p><strong>master</strong>：主控节点 </p>\n<ul>\n<li>API Server：集群统一入口，以restful风格进行操作，同时交给etcd存储 </li>\n<li>scheduler：节点的调度，选择node节点应用部署</li>\n<li>controller-manager：处理集群中常规后台任务，一个资源对应一个控制器</li>\n<li>etcd：存储系统，用于保存集群中的相关数据</li>\n</ul>\n</li>\n<li><p><strong>Work node</strong>：工作节点 </p>\n<ul>\n<li>Kubelet：master派到node节点代表，管理本机容器 <ul>\n<li>一个集群中每个节点上运行的代理，它保证容器都运行在Pod中</li>\n<li>负责维护容器的生命周期，同时也负责Volume(CSI) 和 网络(CNI)的管理</li>\n</ul>\n</li>\n<li>kube-proxy：提供网络代理，负载均衡等操作</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-e031b4aedb48753cf3d2e88c741166f1_r.jpg\"></p>\n<h2 id=\"搭建集群\"><a href=\"#搭建集群\" class=\"headerlink\" title=\"搭建集群\"></a>搭建集群</h2><h3 id=\"搭建方式\"><a href=\"#搭建方式\" class=\"headerlink\" title=\"搭建方式\"></a>搭建方式</h3><ul>\n<li>单master集群</li>\n<li>多master集群</li>\n</ul>\n<h3 id=\"部署方式\"><a href=\"#部署方式\" class=\"headerlink\" title=\"部署方式\"></a>部署方式</h3><ul>\n<li><p>Kubeadm</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><ul>\n<li>kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署： <ul>\n<li>创建一个Master 节点kubeadm init</li>\n<li>将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt;</li>\n</ul>\n</li>\n<li>原理：<ul>\n<li>把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。</li>\n</ul>\n</li>\n</ul></blockquote>\n</li>\n<li><p>二进制搭建</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><ul>\n<li><p>从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。</p>\n<p>Kubeadm降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。</p>\n</li>\n</ul></blockquote>\n</li>\n</ul>\n<h3 id=\"相关工具\"><a href=\"#相关工具\" class=\"headerlink\" title=\"相关工具\"></a>相关工具</h3><p>kubectl</p>\n<ul>\n<li>kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署</li>\n</ul>\n<p>YAMl</p>\n<ul>\n<li>YAML文件：就是资源清单文件，用于资源编排 </li>\n<li>k8s 集群中对资源管理和资源对象编排部署都可以通过声明样式（YAML）文件来解决，也就是可以把需要对资源对象操作编辑到YAML 格式文件中，我们把这种文件叫做资源清单文件，通过kubectl 命令直接使用资源清单文件就可以实现对大量的资源对象进行编排部署了。一般在我们开发的时候，都是通过配置YAML文件来部署集群的。</li>\n</ul>\n<h2 id=\"相关操作\"><a href=\"#相关操作\" class=\"headerlink\" title=\"相关操作\"></a>相关操作</h2><ul>\n<li><p>编写YAML文件</p>\n<ul>\n<li><p>YAML文件本质上是一个配置文件，用来把容器的定义、参数、配置，统统记录在一个 YAML 文件中 ，对于K8s而言，YAML是API Object </p>\n</li>\n<li><p>具体格式：</p>\n<ul>\n<li><p>主要分为四大部分</p>\n<ul>\n<li>apiVersion   <strong>api版本</strong></li>\n<li>kind              <strong>API对象种类</strong></li>\n<li>metadata    <strong>对象标识数据</strong></li>\n<li>spec              <strong>对象功能描述</strong></li>\n</ul>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">apiVersion: apps&#x2F;v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n  spec:\n    containers:\n      - name: nginx\n      image: nginx:1.7.9\n      ports:\n        - containerPort: 80</code></pre></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>相关命令</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">$ kubectl create -f nginx-deployment.yaml       &#x2F;&#x2F;运行YAML文件</code></pre>\n\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">$ kubectl get pods -l app&#x3D;nginx &#x2F;&#x2F;查看运行的pod</code></pre>\n\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">kubectl describe pod nginx-deployment-67594d6bf6-9gdvr &#x2F;&#x2F;查看对象的详细信息\n&#x2F;&#x2F;其中的events记录了操作pod的重要事件，经常用于debug</code></pre></li>\n</ul>\n<h2 id=\"核心概念\"><a href=\"#核心概念\" class=\"headerlink\" title=\"核心概念\"></a>核心概念</h2><h3 id=\"资源以及相应控制\"><a href=\"#资源以及相应控制\" class=\"headerlink\" title=\"资源以及相应控制\"></a>资源以及相应控制</h3><h4 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h4><ul>\n<li><p>是什么</p>\n<ul>\n<li>是k8s的最小的单位级，里面通过有单个或者多个容器构成，<strong>Pod是它们的逻辑主机</strong>，Pod包含业务相关的多个应用容器。</li>\n</ul>\n</li>\n<li><p>实现原理</p>\n<ul>\n<li><p>pod本质上是一个<strong>逻辑上的集合体</strong>，并不是具体使用相关隔离和限制技术创建的一个空间</p>\n</li>\n<li><p>实现逻辑体的原理：</p>\n<ul>\n<li>共享网络命名空间（networkname space）<ul>\n<li><strong>在pod中，不同的容器间属于对等关系，而不是从属关系，所以所有的容器只要共享Infra容器的网络命名空间即可</strong></li>\n<li>其中Infra容器是永远处于暂停状态的特殊容器，和pod声明周期进行了绑定</li>\n</ul>\n</li>\n<li>共享存储卷<ul>\n<li>在创建pod的时候声明volume即可</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>为什么是pod而不是容器</p>\n<ul>\n<li>pod的存在主要是让几个紧密连接的几个容器之间共享资源 <strong>（进程间存在亲密性）</strong>如果以容器为单位则管理开销很大</li>\n<li><strong>容器的设计模式</strong>（pod设计的核心思想！！！）<ul>\n<li>理解：相当于设计一个组合拳，用来解决单个容器难以解决的问题 </li>\n<li>sidecar模式：sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进 程（主容器）之外的工作。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>pod设计的核心思想</strong>：</p>\n<ul>\n<li>pod的本质设计<strong>是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用 户程序。</strong> </li>\n<li>是<strong>一种传统应用架构到微服务架构的一种过渡</strong><ul>\n<li>应用从物理机到虚拟机，因为只是底层资源发生了池化，所以迁移的架构变化不大，但是在云原生时代，不能把容器看成一个更加轻量化的虚拟机，因为容器的本质就是一个进程，而虚拟机理往往包含了多个进程，所以此阶段应用的上云不能只是套模板，而是根据容器的特点进行设计，而不仅仅是将虚拟机装进容器。而<strong>pod可以理解为云原生的虚拟机</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>pod相关属性</p>\n<ul>\n<li><p>apiserver：</p>\n</li>\n<li><p>kind</p>\n</li>\n<li><p>metadata</p>\n</li>\n<li><p>spec：</p>\n<ul>\n<li>NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段 </li>\n<li>NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调 度，调度的结果就是赋值的节点名字。 </li>\n<li>HostAliases：定义了 Pod 的 hosts 文件（比如 &#x2F;etc&#x2F;hosts）里的内容 </li>\n<li>shareProcessNamespace&#x3D;true： 容器之间共享相关命名空间</li>\n<li>hostNetwork: true    hostIPC: true    hostPID: true    容器共享宿主机的 Namespace </li>\n<li>Init Container ：</li>\n<li>Container ：</li>\n<li>name：</li>\n<li>image</li>\n<li>port</li>\n<li>ImagePullPolicy ：它定义了镜像拉取的策略</li>\n</ul>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">- IfNotPresent：默认值，镜像在宿主机上不存在才拉取\n- Always：每次创建Pod都会重新拉取一次镜像\n- Never：Pod永远不会主动拉取这个镜像</code></pre>\n\n<ul>\n<li>lifecycle: <ul>\n<li>postStart:  在容器启动后，立刻执行一个指定的操作 （不保证严格顺序）</li>\n<li>preStop:  容器被杀死之前 ，立刻执行一个指定的操作 （preStop 操作的执行，是同步的）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>status：（pod的状态，声明周期）</p>\n<ul>\n<li>phase ：<strong>（pod的当前状态）</strong><ul>\n<li>Pending：这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被 创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比 如，调度不成功。 </li>\n<li>Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创 建成功，并且至少有一个正在运行中。 </li>\n<li>Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情 况在运行一次性任务时最为常见。 </li>\n<li>Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状 态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 </li>\n<li>Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube\u0002apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。</li>\n</ul>\n</li>\n<li>Conditions ：<strong>（描述造成当前 Status 的具体原因）</strong><ul>\n<li>PodScheduled</li>\n<li>Ready </li>\n<li>Initialized</li>\n<li>Unschedulable。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Podpreset</p>\n<ul>\n<li>是什么？<ul>\n<li>就是事先定义好的pod，用来追加新建pod的内容，</li>\n</ul>\n</li>\n<li>目的：<ul>\n<li>减少新建pod的复杂度，批量化修改pod</li>\n</ul>\n</li>\n<li>注意：<ul>\n<li>它只能修改新建的pod的内容，而对控制器内容无法改变</li>\n<li>当有多个podpreset时，新建的pod会将他们的字段进行融合使用</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Pod资源调度限制</p>\n<ul>\n<li>request：表示调度所需的资源</li>\n<li>limits：表示最大所占用的资源<ul>\n<li>只有满足request的要求才能被调用</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Pod的重启策略（恢复机制）</p>\n<ul>\n<li>Always：当容器终止退出后，总是重启容器，默认策略 【nginx等，需要不断提供服务】</li>\n<li>OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。</li>\n<li>Never：当容器终止退出，从不重启容器 【批量任务】</li>\n</ul>\n</li>\n<li><p>Pod的健康检查</p>\n<ul>\n<li>通过探针技术<ul>\n<li>探针种类<ul>\n<li>livenessProbe</li>\n<li>指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。</li>\n<li>readinessProbe<ul>\n<li>指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。</li>\n</ul>\n</li>\n<li>startupProbe<ul>\n<li>指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其 重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>检测结果<ul>\n<li>Success（成功）<ul>\n<li>容器通过了诊断。</li>\n</ul>\n</li>\n<li>Failure（失败）<ul>\n<li>容器未通过诊断。</li>\n</ul>\n</li>\n<li>Unknown（未知）<ul>\n<li>诊断失败，因此不会采取任何行动。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>探针的检查机制<ul>\n<li>exec<ul>\n<li>在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。</li>\n</ul>\n</li>\n<li>grpc<ul>\n<li>使用 gRPC 执行一个远程过程调用。 目标应该实现 gRPC健康检查。 如果响应的状态是 “SERVING”，则认为诊断成功。 gRPC 探针是一个 alpha 特性，只有在你启用了 “GRPCContainerProbe” 特性门控时才能使用。</li>\n</ul>\n</li>\n<li>httpGet<ul>\n<li>对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。</li>\n</ul>\n</li>\n<li>tcpSocket<ul>\n<li>对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Pod的资源调度策略</p>\n<ul>\n<li><p>根据资源的调度限制request进行调配</p>\n</li>\n<li><p>根据nodeSelector标签进行调度</p>\n</li>\n<li><p>根据点亲和性 <strong>nodeAffinity</strong> </p>\n<ul>\n<li>硬亲和性：约束条件必须满足</li>\n<li>软亲和性：尝试满足，不保证</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>污点和污点容忍</p>\n<ul>\n<li><p>概述：</p>\n<p>在调度pod的时候，有request，nodeSelector和nodeAffinity对nod的策略调度，但是pod最终归属是在某个node节点上，所以也可以通过对node的属性进行相关设置，从而侧面实现pod的调度</p>\n<ul>\n<li>Taint 污点：节点不做普通分配调度，是节点属性</li>\n</ul>\n</li>\n<li><p>使用场景</p>\n<ul>\n<li>最大的应用就是master节点，因为里面不能有pod</li>\n<li>专用节点【限制ip】</li>\n<li>配置特定硬件的节点【固态硬盘】</li>\n<li>基于Taint驱逐【在node1不放，在node2放】</li>\n</ul>\n</li>\n<li><p>相关操作</p>\n<ol>\n<li><p>查看污点情况</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">kubectl describe node k8smaster | grep Taint</code></pre>\n\n<ul>\n<li>NoSchedule：一定不被调度</li>\n<li>PreferNoSchedule：尽量不被调度【也有被调度的几率】</li>\n<li>NoExecute：不会调度，并且还会驱逐Node已有Pod</li>\n</ul>\n</li>\n<li><p>如何对node添加污点</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">kubectl taint node [node] key&#x3D;value:污点的三个值</code></pre>\n</li>\n<li><p>删除污点</p>\n</li>\n</ol>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">kubectl taint node k8snode1 env_role:NoSchedule-</code></pre></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Controller\"><a href=\"#Controller\" class=\"headerlink\" title=\"Controller\"></a>Controller</h4><ul>\n<li><p>是什么</p>\n<p>Controller是在集群上<strong>管理和运行pod的对象</strong> </p>\n</li>\n<li><p>Pod和Controller的关系</p>\n<ul>\n<li>controller就是pod的控制器，用来控制pod的相关功能，比如回滚，    弹性伸缩等等</li>\n<li>Pod 和 Controller之间是通过label标签来建立关系，同时Controller又被称为控制器工作负载</li>\n</ul>\n</li>\n<li><p>种类</p>\n<ul>\n<li><p>deployment</p>\n</li>\n<li><p>Statefulset</p>\n</li>\n<li><p>DaemonSet</p>\n</li>\n<li><p>Job</p>\n</li>\n<li><p>CronJob</p>\n</li>\n<li><p>Replication Controller</p>\n</li>\n<li><p>Replica Set</p>\n</li>\n</ul>\n</li>\n<li><p>Deployment</p>\n<ul>\n<li>概述<ul>\n<li>是controller的一种</li>\n</ul>\n</li>\n<li>作用：<ul>\n<li>pod的<strong>水平扩展&#x2F;收缩</strong>和<strong>滚动更新</strong></li>\n<li>Deployment控制器可以部署<strong>无状态应用</strong></li>\n</ul>\n</li>\n<li>原理：<ul>\n<li>Deployment控制着RS的版本，而RS控制着Pod副本的数量</li>\n<li><strong>水平扩展&#x2F;收缩：</strong>RS控制</li>\n<li><strong>滚动更新：</strong>两个RS交替更替</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Statefulset</p>\n<ul>\n<li>主要用于部署<strong>有状态的pod</strong></li>\n<li>适合StatefulSet的业务包括数据库服务MySQL 和 PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务</li>\n</ul>\n</li>\n<li><p>DaemonSet</p>\n<ul>\n<li>DaemonSet 即后台支撑型服务，主要是用来部署守护进程 </li>\n<li>典型的后台支撑型服务包括：存储、日志和监控等。在每个节点上支撑K8S集群运行的服务。</li>\n</ul>\n</li>\n<li><p>Job</p>\n<ul>\n<li>Job也即一次性任务 </li>\n<li>Job管理的Pod根据用户的设置把任务成功完成就自动退出了。</li>\n</ul>\n</li>\n<li><p>CronJob</p>\n<ul>\n<li>定时任务</li>\n</ul>\n</li>\n<li><p>Replication Controller</p>\n<ul>\n<li>RC是K8S中较早期的技术概念，只适用于长期伺服型的业务类型，比如控制Pod提供高可用的Web服务。</li>\n</ul>\n</li>\n<li><p>Replica Set</p>\n<ul>\n<li>Replica Set 检查 RS，也就是副本集。RS是新一代的RC，提供同样高可用能力，区别主要在于RS后来居上，能够支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数来使用</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"自定义资源以及相应控制\"><a href=\"#自定义资源以及相应控制\" class=\"headerlink\" title=\"自定义资源以及相应控制\"></a>自定义资源以及相应控制</h3><h4 id=\"声明式API\"><a href=\"#声明式API\" class=\"headerlink\" title=\"声明式API\"></a>声明式API</h4><ul>\n<li><p>个人理解：</p>\n<ul>\n<li>声明式API是K8s的核心，因为互联网的规模很大，仅仅使用K8s内部的编排对象可能不够用，所以要自己编写API对象和编排对象进行适合业务的资源控制，而这种功能的实现依靠的是K8s的API插拔机制，可以理解为在不修改K8s的基础上进行功能的扩展，所以我们编写API对象和编排对象都是通过声明式API进行编写，而通过API插拔机制进行实现，而这也是<strong>k8s使用者到k8s玩家的一个重要过渡</strong></li>\n</ul>\n</li>\n<li><p>请求</p>\n<ul>\n<li>响应式</li>\n<li><strong>声明式</strong><ul>\n<li>理解：声明式API关注的核心是状态是否正常，而不是一步步琐碎的执行相关过程。只关心结果，不关心过程，过程由k8s自动解决</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>是什么？</p>\n<ul>\n<li>是 kubectl apply 命令。 YAML只是个配置文件</li>\n<li>kubectl apply，执行了一个对原有 API 对象的 PATCH 操 作。 </li>\n<li>请求分类：<ul>\n<li>响应命令式请求 ：一次只能处理一个写请求，否则会有产生冲突的可能 （kubectl replace ）</li>\n<li><strong>声明式请求</strong> ：一次能处理多个写操作，并且具备 Merge 能力。 （kubectl apply）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>声明式 API 的设计 </p>\n<ul>\n<li>API 对象在 Etcd 里的完整资源路径 <ul>\n<li>Group（API 组） </li>\n<li>Version（API 版本） </li>\n<li>Resource（API 资源类型）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>声明式 API 的创建流程</p>\n<ol>\n<li>创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给 了 APIServer。 </li>\n<li>过滤这个请求，并完成一些前置性的工作，比如授权、超时 处理、审计等。 </li>\n<li>进入 MUX 和 Routes 流程 <ul>\n<li>MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所 <ul>\n<li>Handler就是完成对象的绑定<ul>\n<li>Kubernetes 对 Resource、Group 和 Version 进行解析，从而在 Kubernetes 项 目里<strong>找到 CronJob 对象的定义</strong> <ul>\n<li>Kubernetes 会匹配 API 对象的组 </li>\n<li>Kubernetes 会进一步匹配到 API 对象的版本号 </li>\n<li>Kubernetes 会匹配 API 对象的资源类型</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。 <ul>\n<li>进行一个 Convert 工作 <ul>\n<li>把用户提交的 YAML 文件，转换 成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提 交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。</li>\n</ul>\n</li>\n<li>APIServer 会先后进行 Admission() 和 Validation() 操作 <ul>\n<li>Admission()</li>\n<li>Validation()  <ul>\n<li>Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的 API 对象，都保 存在了 APIServer 里一个叫作 Registry 的数据结构中。也就是说，只要一个 API 对象的定义能 在 Registry 里查到，它就是一个有效的 Kubernetes API 对象。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调 用 Etcd 的 API 把它保存起来。</li>\n</ol>\n</li>\n<li><p><strong>API 插件机制：CRD</strong> </p>\n<ul>\n<li><p>是什么？</p>\n<ul>\n<li>自定义 API 资源 </li>\n<li>CRD 的全称是 Custom Resource Definition。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型</li>\n</ul>\n</li>\n<li><p>个人理解：</p>\n<ul>\n<li>尽管K8s自带的一些资源定义已经够用，但是在具体的使用上，需要量身定制一些“自定义资源”，也就是CR（用户自定义资源），而CR的定义就是我们要的的CRD</li>\n</ul>\n</li>\n<li><p>如何编写：</p>\n<ol>\n<li><p><strong>创建API对象：</strong></p>\n<ul>\n<li><ul>\n<li><ol>\n<li><p>进行定义：</p>\n<ul>\n<li><ol>\n<li><p>在K8s进行宏观的定义</p>\n<ul>\n<li>编写YAML文件</li>\n</ul>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">apiVersion: apiextensions.k8s.io&#x2F;v1beta1\nkind: CustomResourceDefinition\nmetadata:\n name: networks.samplecrd.k8s.io\nspec:\n group: samplecrd.k8s.io\n version: v1\n names:\n kind: Network\n plural: networks\n scope: Namespaced\n</code></pre>\n</li>\n<li><p>用Go语言进行微观的定义</p>\n<ul>\n<li>用GO创建相关文件夹<ul>\n<li>pkg&#x2F;apis&#x2F;samplecrd 目录下创建了一个 register.go 文件，用来放置后面要用到的 全局变量 </li>\n<li>pkg&#x2F;apis&#x2F;samplecrd 目录下添加一个 doc.go 文件 ，起到的是全局的代码生成控制的作用，所以也被称 为 Global Tags。 </li>\n<li>pkg&#x2F;apis&#x2F;samplecrd 目录下添加一个 添加 types.go 文件。顾名思义，它的作用就是定义一个 Network 类型到底有 哪些字段（比如，spec 字段里的内容） <ul>\n<li>除了定义 Network 类型，你还需要定义一个 NetworkList 类型</li>\n</ul>\n</li>\n<li>编写的一个 pkg&#x2F;apis&#x2F;samplecrd&#x2F;v1&#x2F;register.go 文件 <ul>\n<li>registry 的作用就是注册一个类型 （Type）到 APIServer 中。而这个内层目录下的 register.go，就是这个注册流程要使用的代 码。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p>使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动 生成 clientset、informer 和 lister</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>为这个 API 对象编写一个自定义控制器（Custom Controller）</strong>    这一部分要结合具体的场景进行深刻的认识</p>\n<ul>\n<li>编写 main 函数 </li>\n<li>main 函数的主要工作就是，定义并初始化一个自定义控制器（Custom Controller），然后启 动它。 <ul>\n<li>第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路 径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。 </li>\n<li>第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即： networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控 制器。 </li>\n<li>第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。</li>\n</ul>\n</li>\n<li>编写自定义控制器的定义 </li>\n<li>及编写控制器里的业务逻辑</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"授权\"><a href=\"#授权\" class=\"headerlink\" title=\"授权\"></a>授权</h4><ul>\n<li><p>背景：</p>\n<ul>\n<li>Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操 作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作 ，在 Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC：基于角色的访问控制（Role-Based Access Control）。</li>\n</ul>\n</li>\n<li><p>为什么要用RBAC？</p>\n<ul>\n<li>Kubernetes 中所有的 API 对象，都保存在 Etcd 里，当API对象进入K8s工作的时候统一和APIserver进行打交道，而也会涉及到权限的控制，而RABC就是实现API对象的权限控制，也就是授权工作</li>\n</ul>\n</li>\n<li><p><strong>基于角色的访问控制</strong>（RBAC）：</p>\n<ul>\n<li><p>核心概念：</p>\n<ul>\n<li><p>Role：角色，<strong>它其实是一组规则</strong>，定义了一组对 Kubernetes API 对象的操作权限。 </p>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">kind: Role\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nmetadata:\n namespace: mynamespace\n name: example-role\nrules:\n- apiGroups: [&quot;&quot;]\n resources: [&quot;pods&quot;]\n verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</code></pre>\n\n\n</li>\n<li><p>Subject：被作用者，既可以是“人”，也可以是“机器”，也可以使你在 Kubernetes 里 定义的“用户” </p>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">apiVersion: v1\nkind: ServiceAccount\nmetadata:\n namespace: mynamespace\n name: example-sa</code></pre>\n\n\n\n\n</li>\n<li><p>RoleBinding：定义了“被作用者”和“角色”的绑定关系。 </p>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">kind: RoleBinding\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nmetadata:\n name: example-rolebinding\n namespace: mynamespace\nsubjects:\n- kind: User\n name: example-user\n apiGroup: rbac.authorization.k8s.io\nroleRef:\n kind: Role\n name: example-role\n apiGroup: rbac.authorization.k8s.io</code></pre></li>\n</ul>\n</li>\n<li><p>个人理解：</p>\n<ul>\n<li>role的本质是一种权限的规则，Subject是被role生效的对象，而RoleBinding就像钩子一样将二者进行连接。</li>\n<li>其中的subject有很多种，比如，User、Group 等 ，但在我们平常的使用中，最 普遍的用法还是 ServiceAccount。 而它是Kubernetes 里的“内置用户”。 </li>\n<li>而我们最终使用的是在API对象中声明ServiceAccount，对API对象进行权限的生效。</li>\n</ul>\n</li>\n<li><p>原理：</p>\n<ul>\n<li>当三者创建好后，ServiceAccount会创建secret的对象，这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，<strong>我们一般称它为：Token。</strong>Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式 保存在 Etcd 当中。</li>\n</ul>\n</li>\n<li><p>默认配置：</p>\n<ul>\n<li>如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名 叫 default 的默认 ServiceAccount，然后分配给这个 Pod。 但在这种情况下，这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。</li>\n</ul>\n</li>\n<li><p>扩展：</p>\n<ul>\n<li>基本的三者是基于namespace里进行生效的，而<strong>ClusterRole</strong> 和 <strong>ClusterRoleBinding</strong>，则是 Kubernetes 集群级别的 Role 和 RoleBinding，它们的作用范围不受 Namespace 限制。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Operator-管理有状态应用\"><a href=\"#Operator-管理有状态应用\" class=\"headerlink\" title=\"Operator (管理有状态应用)\"></a>Operator (管理有状态应用)</h4><ul>\n<li>是什么<ul>\n<li>用来管理有状态应用的一个工具</li>\n</ul>\n</li>\n<li>理解：<ul>\n<li>本质上是一个外接的API对象，所以要进行RBAC，而他在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定 义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"存储\"><a href=\"#存储\" class=\"headerlink\" title=\"存储\"></a>存储</h3><h3 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h3><h4 id=\"容器网络\"><a href=\"#容器网络\" class=\"headerlink\" title=\"容器网络\"></a>容器网络</h4><ul>\n<li><p>网络模式</p>\n<ul>\n<li>bridge<ul>\n<li>桥接模式：所有隔离的docker容器通过Veth Pair 统一和docker0这个网桥进行连接来实现互通（Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出 现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网 卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。 这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。 ）</li>\n</ul>\n</li>\n<li>host<ul>\n<li>容器和宿主机公用网络</li>\n</ul>\n</li>\n<li>none<ul>\n<li>没有网络配置</li>\n</ul>\n</li>\n<li>container<ul>\n<li>容器之间公用网络配置</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>跨主机通信</strong></p>\n<ul>\n<li><p>Flannel  项目来解决跨机通信（Overlay）</p>\n<ul>\n<li><p>是什么？</p>\n<ul>\n<li>是一个框架用来解决容器的网络方案。</li>\n</ul>\n</li>\n<li><p>实现方式：</p>\n<ul>\n<li>VXLAN </li>\n<li>host-gw </li>\n<li>UDP</li>\n</ul>\n</li>\n<li><p>实现方式详解：</p>\n<ul>\n<li><p>UDP</p>\n<p><img src=\"https://s3.bmp.ovh/imgs/2022/05/05/8cd519614e8dae66.jpg\"></p>\n<ul>\n<li>flannel：<ul>\n<li>flannel0：<ul>\n<li>它是一个 TUN 设备（Tunnel 设备） ，在操作系统内核和用户应用程序之间传递 IP 包 ，存在于内核态中</li>\n</ul>\n</li>\n<li>flanneld<ul>\n<li>在用户态，接收flannel0传递的IP包</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>步骤概述：<ol>\n<li>容器正常发送数据包（用户态–核心态）</li>\n<li>flannel0接收到数据后传输到flanneld（核心态–用户态）</li>\n<li>flanneld使用UDP封装发送（用户态–核心态）</li>\n</ol>\n</li>\n<li>缺点：<ul>\n<li>因为使用该方式发送数据时，会多次经历内核态和用户态的转换，所以性能比较差，无法大规模使用。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>VXLAN</p>\n<ul>\n<li><p>核心思想</p>\n<ul>\n<li>在三层上构建大二层来实现跨机通信（在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可 以）之间，可以像在同一个局域网（LAN）里那样自由通信。 ）</li>\n</ul>\n</li>\n<li><p>核心概念</p>\n<ul>\n<li>VTEP（虚拟隧道端点 ）<ul>\n<li>理解：相当于连通大二层网络隧道的端口，它进行封装和解封装的对象，是二层数据帧（Ethernet frame） ，而VTEP设备相当于建立大二层网络的每个连通的站点。</li>\n<li>如何获得目的VTEP设备信息：<ul>\n<li>VTEP设备的信息由每台宿主机上的 flanneld 进程负责维护，会对其他节点进行类似的宣告，从而得到信息。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>步骤概述：<ol>\n<li>容器发送数据包</li>\n<li>数据包在VTEP进行封装成新的<strong>数据帧</strong>———构建的二层</li>\n<li>数据帧再正常的进行封装成在网络传输的数据进行传输————依托于基础的三层</li>\n</ol>\n</li>\n</ul>\n<p><img src=\"https://s3.bmp.ovh/imgs/2022/05/05/c1f3c41e1decbb2d.jpg\"></p>\n</li>\n</ul>\n</li>\n<li><p>host-gw </p>\n<ul>\n<li>就是将宿主机当做网关构成三层通信网络</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"K8s网络\"><a href=\"#K8s网络\" class=\"headerlink\" title=\"K8s网络\"></a>K8s网络</h4><ul>\n<li><p>背景：</p>\n<ul>\n<li>跨机通信的核心是“ 跨机”，而在flannel项目中通过网络插件的思路（UDP中的flennel0和VXLAN的VTEP）来实现网络主机的连接，而在K8s中，也是使用同样的思路。而使用的插件是CNI。</li>\n</ul>\n</li>\n<li><p>CNI实现</p>\n<ul>\n<li>Kubernetes 是通过一个叫作 CNI 的接口，<strong>维护了一个单独的网桥来代替 docker0</strong>。这个网桥 的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。 </li>\n<li>为什么要用CNI？<ul>\n<li>在K8s中，并不是围绕docker进行开发和设计，而k8s中的最小单位是pod，所以docker的桥接bridge模式就不是很合适，另一方面是因为pod中的容器都会连接到pause容器中，所以CNI的主要服务对象是pause容器进而来进行网络的配置。</li>\n</ul>\n</li>\n<li>设计思想：<ul>\n<li>CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络 插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。</li>\n</ul>\n</li>\n<li>CNI的基础可执行文件 ：<ul>\n<li>第一类，叫作 Main 插件 ，它是用来创建具体网络设备的二进制文件。 </li>\n<li>第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文 件。 </li>\n<li>第三类，是由 CNI 社区维护的内置 CNI 插件。比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件</li>\n</ul>\n</li>\n<li>CNI 插件的部署和实现方式</li>\n</ul>\n<ol>\n<li>实现这个网络方案本身 <ul>\n<li>这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑 ，<strong>简单来说就是配置相关网络环境</strong></li>\n</ul>\n</li>\n<li>实现该网络方案对应的 CNI 插件 <ul>\n<li>这一部分主要需要做的，就是配置 Infra 容器里面的 网络栈，并把它连接在 CNI 网桥上。 <strong>具体功能的实现</strong></li>\n</ul>\n</li>\n<li>在宿主机上安装 flanneld（网络方案本身） <ul>\n<li>flanneld 启 动后会在每台宿主机上生成它对应的CNI 配置文件（它其实是一个 ConfigMap），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案。</li>\n</ul>\n</li>\n<li>dockershim 会加载上述的 CNI 配置文件</li>\n</ol>\n<ul>\n<li><p>CNI 插件的工作原理 </p>\n</li>\n<li><p>Flannel CNI 插件 </p>\n</li>\n<li><p>CNI bridge  插件</p>\n<ul>\n<li>CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在 ，如果没有的话，那就创建它 。</li>\n<li>CNI bridge 插件会通过 Infra 容器的 Network Namespace 文件，进入到这个 Network Namespace 里面，然后创建一对 Veth Pair 设备。 </li>\n<li>紧接着，它会把这个 Veth Pair 的其中一端，“移动”到宿主机上。 </li>\n<li>CNI bridge 插件还会为它设置Hairpin Mode（发夹模式） </li>\n<li>CNI bridge 插件会调用 CNI ipam 插件，从 ipam.subnet 字段规定的网段里为容器分 配一个可用的 IP 地址。然后，CNI bridge 插件就会把这个 IP 地址添加在容器的 eth0 网卡 上，同时为容器设置默认路由。 </li>\n<li>CNI 插件会把容器的 IP 地址等信息返回给 dockershim，然后被 kubelet 添加到 Pod 的 Status 字段</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>三层网络实现：</p>\n<ul>\n<li>host-gw模式</li>\n<li>calico项目</li>\n</ul>\n</li>\n<li><p>网络隔离 </p>\n<ul>\n<li>背景：<ul>\n<li>网络既需要连通，也需要隔离，个网络的隔离能力，是依靠一种专门的 API 对象来描述的，即： NetworkPolicy。</li>\n</ul>\n</li>\n<li>NetworkPolicy<ul>\n<li>是什么？<ul>\n<li>API对象，用来网络隔离；</li>\n</ul>\n</li>\n<li>podSelector 字段 ：</li>\n<li>用来限制网络范围，为空时，NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod </li>\n<li>效果：<ul>\n<li>一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状 态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"服务发现\"><a href=\"#服务发现\" class=\"headerlink\" title=\"服务发现\"></a>服务发现</h3><h4 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a><strong>Service</strong></h4><ul>\n<li><p>背景;</p>\n<ul>\n<li>Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一 组 Pod 实例之间总会有负载均衡的需求。 </li>\n<li>所以service相当于pods之间的一个代理，用来服务发现和负载均衡</li>\n<li>所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。</li>\n</ul>\n</li>\n<li><p>分类：</p>\n<ul>\n<li>内部访问<ul>\n<li>clusterIP</li>\n</ul>\n</li>\n<li>外部访问<ul>\n<li>NodePort</li>\n<li>LoadBalancer 和</li>\n<li>External Name</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>实现原理：</p>\n<ul>\n<li><p>浅层实现：</p>\n<ul>\n<li><p>service这个api资源对象通过selector标签和相应的pod进行绑定，而被service绑定的pod称为Service 的 Endpoints</p>\n<ul>\n<li><p>注意：</p>\n<p>只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>本质实现：<strong>Service 是由 kube-proxy 组件，加上 iptables或者IPVS 来共同实现的。</strong> </p>\n<ul>\n<li>iptables实现：<ul>\n<li>其实Service的功能就类似与NAT的功能<ol>\n<li>提交service后，kube—proxy创建一条iptable规则（而这个规则就是一个映射，由VIP到iptables链的一个映射）</li>\n<li>iptables链本质上就是一组规则的集合，一组DNAT的规则，用来对用不同pod之间ip信息的转化，最终实现功能</li>\n</ol>\n</li>\n<li>缺点：<ul>\n<li>因为每个pod需要对应相应的iptable规则，所以当pod数量大之后，需要消耗大量资源来进行维护iptables。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>IPVS实现：<ul>\n<li><a href=\"https://www.jianshu.com/p/89f126b241db\">k8s集群中ipvs负载详解 - 简书 (jianshu.com)</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"负载均衡\"><a href=\"#负载均衡\" class=\"headerlink\" title=\"负载均衡\"></a>负载均衡</h3><h4 id=\"Ingress\"><a href=\"#Ingress\" class=\"headerlink\" title=\"Ingress\"></a>Ingress</h4><ul>\n<li>是什么<ul>\n<li>全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。 </li>\n<li>所谓 Ingress，就是 <strong>Service 的“Service”。</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"资源管理和调度\"><a href=\"#资源管理和调度\" class=\"headerlink\" title=\"资源管理和调度\"></a>资源管理和调度</h3><ul>\n<li><h4 id=\"资源模型\"><a href=\"#资源模型\" class=\"headerlink\" title=\"资源模型\"></a>资源模型</h4><ul>\n<li>背景：<ul>\n<li>计算机的计算最大的两个资源是 cpu 和 内存，所以对响应pod资源的限制也是从这两个方向入手</li>\n</ul>\n</li>\n<li>CPU和内存<ul>\n<li>CPU<ul>\n<li>CPU 这样的资源被称作“可压缩资源”（compressible resources）。 它的典型特点是，当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。 </li>\n<li>具体利用率设置 全部为1000m，使用多少就设置响应多少 xxm</li>\n</ul>\n</li>\n<li>内存<ul>\n<li>内存这样的资源，则被称作“不可压缩资源（compressible resources）。当不可压缩资 源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。 </li>\n<li>内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、 Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>limits 和 request<ul>\n<li>背景：<ul>\n<li>Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边 界”的定义，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守 的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。</li>\n</ul>\n</li>\n<li>Limits（最大调度）</li>\n<li>request（最小调度）</li>\n</ul>\n</li>\n<li>Qos模型<ul>\n<li>背景：<ul>\n<li>Qos模型实际上是根据pod设置的不同的limits和request进行的一种分类，而这种分类决定了OOM时候对响应容器杀死的顺序</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>Guaranteed </li>\n<li>Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等 （如果只设置了request，k8s则会默认设置limits，并且和request相等。）</li>\n<li>Burstable </li>\n<li>Pod至少有一个 Container 设置了 requests </li>\n<li>BestEffort <ul>\n<li>Pod 既没有设置 requests，也没有设置 limits</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Eviction（资源回收） <ul>\n<li>是什么<ul>\n<li>就是资源紧张的时候根据Qos模型对Pod中的容器进行回收</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>Soft<ul>\n<li>Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间” ，过了相应时间才进行回收</li>\n</ul>\n</li>\n<li>Hard<ul>\n<li>立刻进行回收</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>回收顺序<ul>\n<li>首当其冲的，自然是 BestEffort 类别的 Pod。</li>\n<li>其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的 Pod。 </li>\n<li>最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时， Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>cpuset<ul>\n<li>是什么<ul>\n<li>是对Pod进行cpu的绑定</li>\n</ul>\n</li>\n<li>作用<ul>\n<li>减少cpu之间切换带来的损耗，提高pod的性能</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><h4 id=\"调度\"><a href=\"#调度\" class=\"headerlink\" title=\"调度\"></a>调度</h4><ul>\n<li><p>默认调度器</p>\n<ul>\n<li><p>背景：</p>\n<ul>\n<li>创建了pod之后，需要安排pod在哪个node上，所以这个安排的好坏很重要，而k8s这个安排的工作就是使用<strong>默认调度器</strong></li>\n</ul>\n</li>\n<li><p>作用：</p>\n<ul>\n<li>默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最 合适的节点（Node）</li>\n</ul>\n</li>\n<li><p>调度结果的本质：</p>\n<ul>\n<li>就是在pod的spec.nodeName    绑定相应的node名字</li>\n</ul>\n<p><img src=\"https://s3.bmp.ovh/imgs/2022/05/05/5eb0a097b4cd6ab4.jpg\"></p>\n</li>\n<li><p>默认调度器架构</p>\n<ul>\n<li>核心：<ul>\n<li>如上图，Kubernetes 的调度器的核心，实际上就是<strong>两个相互独立的控制循环</strong></li>\n</ul>\n</li>\n<li>双循环<ul>\n<li>Informer Path<ul>\n<li>它的主要目的，是启动一系列 Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变 化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器 就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列</li>\n</ul>\n</li>\n<li>Scheduling Path<ul>\n<li>Scheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用 Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个 Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里 直接拿到的，这是调度器保证算法执行效率的主要手段之一。 接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分 最高的 Node，就会作为这次调度的结果    。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>调度流程：</p>\n<ol>\n<li>从集群所有的节点中，根据Predicate 调度算法挑选出所有可以运行该 Pod 的节点    （<strong>筛选出可用的pod</strong>）</li>\n<li>从第一步的结果中，再根据Priority调度算法挑选一个最符合条件的节点作为最终结果 （<strong>从筛选的结果中挑选最优的</strong>）</li>\n</ol>\n</li>\n<li><p>调度策略</p>\n<ul>\n<li>Predicates策略<ul>\n<li>理解：<ul>\n<li>因为该策略的作用主要是筛选，所以该策略可以类比为过滤器，其核心就是过滤规则的设定</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>第一种类型，叫作 GeneralPredicates<ul>\n<li>负责最基础的调度策略，并且可以被引用</li>\n</ul>\n</li>\n<li>第二种类型，是与 Volume 相关的过滤规则<ul>\n<li>负责的是跟容器持久化 Volume 相关的调度策略</li>\n</ul>\n</li>\n<li>第三种类型，是宿主机相关的过滤规则    <ul>\n<li>考察待调度 Pod 是否满足 Node 本身的某些条件</li>\n</ul>\n</li>\n<li>第四种类型，是 Pod 相关的过滤规则    <ul>\n<li>作用和第一类相似，但是多了PodAffinityPredicate特性，检查待调度 Pod 与 Node 上的已有 Pod 之间的亲 密（affinity）和反亲密（anti-affinity）关系</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Priorities 策略<ul>\n<li>理解：<ul>\n<li>就是给相关pod打分的，分高的优先调度</li>\n</ul>\n</li>\n<li>相关策略<ul>\n<li>LeastRequestedPriority    （最小资源）</li>\n<li>ImageLocalityPriority          （镜像相关）</li>\n<li>。。。。。。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>优先和抢占机制</p>\n<ul>\n<li><h4 id=\"背景：\"><a href=\"#背景：\" class=\"headerlink\" title=\"背景：\"></a>背景：</h4><ul>\n<li>正常情况下，当一个 Pod 调度失败后，它就会被暂时“搁置”起来，直到 Pod 被更新，或者集 群状态发生变化，调度器才会对这个 Pod 进行重新调度。 但在有时候，我们希望的是这样一个场景。当一个高优先级的 Pod 调度失败后，该 Pod 并不会 被“搁置”，而是会“挤走”某个 Node 上的一些低优先级的 Pod 。这样就可以保证这个高优 先级 Pod 的调度成功。这个特性，其实也是一直以来就存在于 Borg 以及 Mesos 等项目里的一 个基本功能。 <strong>优先级和抢占机制，解决的是 Pod 调度失败时该怎么办的问题</strong></li>\n</ul>\n</li>\n<li><h4 id=\"优先级\"><a href=\"#优先级\" class=\"headerlink\" title=\"优先级\"></a>优先级</h4><ul>\n<li>关于优先级的大小<ul>\n<li>优先级是一个 32 bit 的整数，最大值不超过 1000000000（<strong>10 亿</strong>，1 billion），并且值越大代表优先级越高。而超出 10 亿的值，其实是被 Kubernetes 保留下来分 配给系统 Pod 使用的。显然，这样做的目的，就是保证系统 Pod 不会被用户抢占掉</li>\n</ul>\n</li>\n<li>PriorityClass<ul>\n<li>作用：<ul>\n<li>该API对象主要是类似于一个配置对象，pod通过priorityClassName    进行绑定，从而设置优先级的值</li>\n</ul>\n</li>\n<li>内容设置：<ul>\n<li>value（优先级设置）</li>\n<li>globalDefault（全局优先级是否开启，通常为false）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>理解：<ul>\n<li>调度器里维护着一个调度队列。所以，当 Pod 拥有了优先 级之后，高优先级的 Pod 就可能会比低优先级的 Pod 提前出队，从而尽早完成调度过程  （<strong>优先级决定了出队列的顺序</strong>）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><h4 id=\"抢占机制\"><a href=\"#抢占机制\" class=\"headerlink\" title=\"抢占机制\"></a>抢占机制</h4><ul>\n<li>背景：<ul>\n<li>当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会 试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后， 待调度的高优先级 Pod 就可以被调度到这个节点上。这个过程，就是“抢占”这个概念在 Kubernetes 里的主要体现    （<strong>就是高优先级pod调度失败后插队被继续调度</strong>）</li>\n</ul>\n</li>\n<li>实现原理：<ul>\n<li>基础条件<ul>\n<li>两个队列，分别为<ul>\n<li>activeQ。凡是在 activeQ 里的 Pod，都是下一个调度周期需要调度的对 象。    </li>\n<li>unschedulableQ，专门用来存放调度失败的 Pod</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>步骤一</strong>：寻找牺牲者<ul>\n<li>第一步，调度器会检查这次失败事件的原因    </li>\n<li>第二步，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使 用这个副本来模拟抢占过程</li>\n</ul>\n</li>\n<li><strong>步骤二</strong>：开始抢占<ul>\n<li>第一步，调度器会检查牺牲者列表，清理这些 Pod 所携带的 nominatedNodeName 字段。</li>\n<li>第二步，调度器会把抢占者的 nominatedNodeName，设置为被抢占的 Node 的名字。</li>\n<li>第三步，调度器会开启一个 Goroutine，异步地删除牺牲者</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>注意事项<ul>\n<li>过程描述<ul>\n<li>抢占过程发生时    ，抢占者并不会立刻被调度到被抢占的 Node 上。事实上，调度器只会 将抢占者的 spec.nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，抢占者 会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上 。</li>\n</ul>\n</li>\n<li>理解：<ul>\n<li>为什么要到下一周期进行调度，而不是立刻抢占？<ul>\n<li>调度器只会通过标准的 DELETE API 来删除被抢占的 Pod，所 以，这些 Pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的 节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退 出期间，集群的可调度性可能会发生的变化（<strong>可以理解为一个对最优抢占的判断，抢占的时机不是只看目前的苟且，而是等下一执行周期后，经过时间的检验，发现这次抢占是最优的，最后进行抢占，而不是立刻进行抢占</strong>）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Istio 项目</p>\n<ul>\n<li><p>是什么</p>\n<ul>\n<li>一个基于 Kubernetes 项目的微服务治理框架</li>\n</ul>\n</li>\n<li><p>核心</p>\n<ul>\n<li>Envoy 容器 （使用的sidecar模式）<ul>\n<li>作用：Envoy 容器就 能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。 </li>\n<li>原理：<ul>\n<li>就是在用户创建pod的时候自动添加相关envoy的信息，进而创建envoy<ul>\n<li>实现原理：<ul>\n<li>首先Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当 中。</li>\n<li>然后通过 Initializer将Envoy的信息加入到pod的API当中<ul>\n<li>该步骤的实现原理：<ul>\n<li>Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。而这个pod的基础镜像是个自定义的控制器，控制器的作用就是为新建的pod打上标签，而这个标签就是在ConfigMap保存的内容</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>原理：</p>\n<ul>\n<li>运行原理：<ul>\n<li>Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。</li>\n</ul>\n</li>\n<li>Dynamic Admission Control。 （动态准入控制）<ul>\n<li>背景：<ul>\n<li>k8s在请求apiserver后会进行初始化，“初始化”操作的实现，借助的是一个叫作 Admission 的功能 （就是一段功能性代码选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。）但是如果要更改Admission的配置要重新编译apiserver，开销很大，所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。</li>\n</ul>\n</li>\n<li>是什么<ul>\n<li>也叫Initializer ，<strong>是热插拔的Admission</strong></li>\n</ul>\n</li>\n<li>依赖：<ul>\n<li>在 Initializer 更新用户的 Pod 对象的时候，必须使用 <strong>PATCH API</strong> 来完成</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","feature":false,"text":"K8s概述前置知识 容器的本质是一种特殊的进程 容器的隔离和资源限制实现 namespace 相当于一个障眼法，利用linux的namespace机制实现隔离，而一个个docker实际上是一个个特殊的进程 Namespace 技术实际上修改了应用进程看待整个计算机“视 图”，即它...","link":"","photos":[],"count_time":{"symbolsCount":"22k","symbolsTime":"20 mins."},"categories":[{"name":"K8s","slug":"K8s","count":4,"path":"api/categories/K8s.json"}],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#K8s\"><span class=\"toc-text\">K8s</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">概述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86\"><span class=\"toc-text\">前置知识</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%98%AF%E4%BB%80%E4%B9%88\"><span class=\"toc-text\">是什么</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B5%85%E6%9E%90K8s\"><span class=\"toc-text\">浅析K8s</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6\"><span class=\"toc-text\">核心组件</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4\"><span class=\"toc-text\">搭建集群</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%90%AD%E5%BB%BA%E6%96%B9%E5%BC%8F\"><span class=\"toc-text\">搭建方式</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F\"><span class=\"toc-text\">部署方式</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7\"><span class=\"toc-text\">相关工具</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">相关操作</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5\"><span class=\"toc-text\">核心概念</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%B5%84%E6%BA%90%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%BA%94%E6%8E%A7%E5%88%B6\"><span class=\"toc-text\">资源以及相应控制</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Pod\"><span class=\"toc-text\">Pod</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Controller\"><span class=\"toc-text\">Controller</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%BA%94%E6%8E%A7%E5%88%B6\"><span class=\"toc-text\">自定义资源以及相应控制</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%A3%B0%E6%98%8E%E5%BC%8FAPI\"><span class=\"toc-text\">声明式API</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%8E%88%E6%9D%83\"><span class=\"toc-text\">授权</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Operator-%E7%AE%A1%E7%90%86%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8\"><span class=\"toc-text\">Operator (管理有状态应用)</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AD%98%E5%82%A8\"><span class=\"toc-text\">存储</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">容器网络</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#K8s%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">K8s网络</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0\"><span class=\"toc-text\">服务发现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Service\"><span class=\"toc-text\">Service</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1\"><span class=\"toc-text\">负载均衡</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Ingress\"><span class=\"toc-text\">Ingress</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%92%8C%E8%B0%83%E5%BA%A6\"><span class=\"toc-text\">资源管理和调度</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B5%84%E6%BA%90%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">资源模型</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B0%83%E5%BA%A6\"><span class=\"toc-text\">调度</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%83%8C%E6%99%AF%EF%BC%9A\"><span class=\"toc-text\">背景：</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%BC%98%E5%85%88%E7%BA%A7\"><span class=\"toc-text\">优先级</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%8A%A2%E5%8D%A0%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">抢占机制</span></a></li></ol></li></ol></li></ol></li></ol>","author":{"name":"Hubert","slug":"blog-author","avatar":"https://tse2-mm.cn.bing.net/th/id/OIP-C.HOHe3l1T_0UEexBraXs53wAAAA?w=169&h=176&c=7&r=0&o=5&dpr=1.38&pid=1.7","link":"/","description":"<h5>This is my own blog to     share my knowledge</h5>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"K8s-核心机制","uid":"2e106d38702b9bd0bc9485bd6f08ef7e","slug":"k8s2","date":"2021-09-26T10:49:36.000Z","updated":"2022-05-05T11:42:43.296Z","comments":true,"path":"api/articles/k8s2.json","keywords":null,"cover":"https://pic4.zhimg.com/v2-562267b2cf39fded4c66640ac37ee818_1440w.jpg?source=172ae18b","text":"浅谈对K8s的理解 最初接触k8s是学完docker后，有了docker这个“集装箱”后，自然而然需要对容器进行编排，所以就开始学习k8s，最开始就觉得k8s就是个运维工具，是个功能强大的运维工具，负责编排容器等工作，以学习一门工具的思维看了k8s调度，网络，存储，控制器等相关的...","link":"","photos":[],"count_time":{"symbolsCount":577,"symbolsTime":"1 mins."},"categories":[{"name":"K8s","slug":"K8s","count":4,"path":"api/categories/K8s.json"}],"tags":[],"author":{"name":"Hubert","slug":"blog-author","avatar":"https://tse2-mm.cn.bing.net/th/id/OIP-C.HOHe3l1T_0UEexBraXs53wAAAA?w=169&h=176&c=7&r=0&o=5&dpr=1.38&pid=1.7","link":"/","description":"<h5>This is my own blog to     share my knowledge</h5>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":false},"next_post":{"title":"Net-OSPF之负载均衡","uid":"0df63ec25452415f0b2c8961aba42a85","slug":"net4","date":"2021-08-02T05:48:25.000Z","updated":"2022-05-05T05:56:59.601Z","comments":true,"path":"api/articles/net4.json","keywords":null,"cover":"https://tse1-mm.cn.bing.net/th/id/R-C.a6b35d0ceff922dbb8ca6678339670c7?rik=OzaOFa32Ojvp7w&riu=http%3a%2f%2fsrc.sotu114.com%2fdata%2fattachment%2fforum%2f202003%2f27%2f145613dpujyuu1v1pztjbd.item.jpg-ture&ehk=EyuUwO0luJ3fTRSq40kKShnm0d5Hwxdx3z56we%2fOoEM%3d&risl=&pid=ImgRaw&r=0","text":"这篇内容主要为了解，具体的功能技术实现自己并未动手操作，但是这篇内容最大的启示是ospf的功能不仅仅是AS内的发现和计算路由，还有其他功能，我们学习一种技术时不能局限于技术本身，而是要善用技术，这样才能真正理解相关技术，从而做到融会贯通 概述： 关于ospf： ​ OSPF(Op...","link":"","photos":[],"count_time":{"symbolsCount":972,"symbolsTime":"1 mins."},"categories":[{"name":"Net","slug":"Net","count":4,"path":"api/categories/Net.json"}],"tags":[],"author":{"name":"Hubert","slug":"blog-author","avatar":"https://tse2-mm.cn.bing.net/th/id/OIP-C.HOHe3l1T_0UEexBraXs53wAAAA?w=169&h=176&c=7&r=0&o=5&dpr=1.38&pid=1.7","link":"/","description":"<h5>This is my own blog to     share my knowledge</h5>","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}